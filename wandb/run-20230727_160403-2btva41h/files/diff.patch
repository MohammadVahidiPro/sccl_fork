diff --git a/main.py b/main.py
index a4845c4..d8c8b64 100644
--- a/main.py
+++ b/main.py
@@ -19,33 +19,44 @@ from utils.kmeans import get_kmeans_centers
 from utils.logger import setup_path, set_global_random_seed
 from utils.optimizer import get_optimizer, get_bert
 import numpy as np
-
+import wandb as wb
 
 def run(args):
-    args.resPath, args.tensorboard = setup_path(args)
-    set_global_random_seed(args.seed)
-
-    # dataset loader
-    train_loader = dataloader.explict_augmentation_loader(args) if args.augtype == "explicit" else dataloader.virtual_augmentation_loader(args)
-
-    # model
-    torch.cuda.set_device(args.gpuid[0])
-    bert, tokenizer = get_bert(args)
-    
-    # initialize cluster centers
-    cluster_centers = get_kmeans_centers(bert, tokenizer, train_loader, args.num_classes, args.max_length)
-    
-    model = SCCLBert(bert, tokenizer, cluster_centers=cluster_centers, alpha=args.alpha) 
-    model = model.cuda()
-
-    # optimizer 
-    optimizer = get_optimizer(model, args)
-    
-    trainer = SCCLvTrainer(model, tokenizer, optimizer, train_loader, args)
-    trainer.train()
-    
-    return None
-
+    with wb.init(project="sccl-2021", config=args) as run:
+        # run.name = TODO:
+        # run.tags =
+        args.resPath, args.tensorboard = setup_path(args)
+        set_global_random_seed(args.seed)
+
+        # dataset loader
+        train_loader = dataloader.explict_augmentation_loader(args) if args.augtype == "explicit" else dataloader.virtual_augmentation_loader(args)
+
+        # model
+        torch.cuda.set_device(args.gpuid[0])
+        bert, tokenizer = get_bert(args)
+        bert.to("cuda")
+        # TODO: assert cuda device 
+        assert next(bert.parameters()).device.type == "cuda"
+        # initialize cluster centers
+        cluster_centers, first_base_scores = get_kmeans_centers(bert, tokenizer, train_loader, args.num_classes, args.max_length, keyword="first-base")
+        wb.run.summary.update(first_base_scores)
+
+        model = SCCLBert(bert, tokenizer, cluster_centers=cluster_centers, alpha=args.alpha) 
+        model = model.cuda()
+        assert next(model.parameters()).device.type == "cuda"
+        assert next(model.contrast_head.parameters()).device.type == "cuda"
+        assert model.cluster_centers.device.type == "cuda"
+
+
+        # optimizer 
+        optimizer = get_optimizer(model, args)
+        trainer = SCCLvTrainer(model, tokenizer, optimizer, train_loader, args)
+        #loss_dic, repre_scores, model_scores = trainer.train()
+        _ , last_base_scores = get_kmeans_centers(bert, tokenizer, train_loader, args.num_classes, args.max_length, keyword="last-base")
+        wb.run.summary.update(last_base_scores)
+        print("##### the final kmeans repre scores: ", last_base_scores)
+        return None
+"""
 def get_args(argv):
     parser = argparse.ArgumentParser()
     parser.add_argument('--train_instance', type=str, default='local') 
@@ -87,13 +98,84 @@ def get_args(argv):
     args.tensorboard = None
 
     return args
+"""
 
+def get_my_args(argv):
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--train_instance', type=str, default='local') 
+    parser.add_argument('--gpuid', nargs="+", type=int, default=[0], help="The list of gpuid, ex:--gpuid 3 1. Negative value means cpu-only")
+    parser.add_argument('--seed', type=int, default=0, help="")
+    parser.add_argument('--print_freq', type=float, default=25, help="")
+    parser.add_argument('--resdir', type=str, default='results/')
+    parser.add_argument('--s3_resdir', type=str, default='results')
+    
+    parser.add_argument('--bert', type=str, default='minilm6', help="")
+    parser.add_argument('--use_pretrain', type=str, default='SBERT', choices=["BERT", "SBERT", "PAIRSUPCON"])
+    
+    # Dataset
+    parser.add_argument('--datapath', type=str, default='datasets/')
+    parser.add_argument('--dataname', type=str, default='searchsnippets', help="")
+    parser.add_argument('--num_classes', type=int, default=8, help="")
+    parser.add_argument('--max_length', type=int, default=32)
+    parser.add_argument('--label', type=str, default='label')
+    parser.add_argument('--text', type=str, default='text')
+    parser.add_argument('--augmentation_1', type=str, default='text1')
+    parser.add_argument('--augmentation_2', type=str, default='text2')
+    # Learning parameters
+    parser.add_argument('--lr', type=float, default=1e-5, help="")
+    parser.add_argument('--lr_scale', type=int, default=100, help="")
+    parser.add_argument('--max_iter', type=int, default=1000)
+    # contrastive learning
+    parser.add_argument('--objective', type=str, default='SCCL')
+    parser.add_argument('--augtype', type=str, default='virtual', choices=['virtual', 'explicit'])
+    parser.add_argument('--batch_size', type=int, default=400)
+    parser.add_argument('--temperature', type=float, default=0.5, help="temperature required by contrastive loss")
+    parser.add_argument('--eta', type=float, default=1, help="")
+    # --temperature 0.5 \
+    #     --eta 10 \
+    #     --lr 1e-05 \
+    #     --lr_scale 100 \
+    #     --max_length 32 \
+    #     --batch_size 400 \
+    #     --max_iter 1000 \
+    #     --print_freq 100 \
+    #     --gpuid 1 &
+    
+    # Clustering
+    parser.add_argument('--alpha', type=float, default=1.0)
+    
+    args = parser.parse_args(argv)
+    args.use_gpu = args.gpuid[0] >= 0
+    args.resPath = None
+    args.tensorboard = None
 
-
+    return args
+"""
+python3 main.py \
+        --resdir $path-to-store-your-results \
+        --use_pretrain SBERT \
+        --bert distilbert \
+        --datapath $path-to-your-data \
+        --dataname searchsnippets \
+        --num_classes 8 \
+        --text text \
+        --label label \
+        --objective SCCL \
+        --augtype virtual \
+        --temperature 0.5 \
+        --eta 10 \
+        --lr 1e-05 \
+        --lr_scale 100 \
+        --max_length 32 \
+        --batch_size 400 \
+        --max_iter 1000 \
+        --print_freq 100 \
+        --gpuid 1 &
+"""
 if __name__ == '__main__':
     import subprocess
        
-    args = get_args(sys.argv[1:])
+    args = get_my_args(sys.argv[1:])
 
     if args.train_instance == "sagemaker":
         run(args)
diff --git a/training.py b/training.py
index 2a8ca7b..7d39fbd 100644
--- a/training.py
+++ b/training.py
@@ -9,10 +9,13 @@ import os
 import time
 import numpy as np
 from sklearn import cluster
-
+import wandb as wb
+from pathlib import Path
 from utils.logger import statistics_log
 from utils.metric import Confusion
 from dataloader.dataloader import unshuffle_loader
+import copy
+
 
 import torch
 import torch.nn as nn
@@ -20,7 +23,9 @@ from torch.nn import functional as F
 from learner.cluster_utils import target_distribution
 from learner.contrastive_utils import PairConLoss
 
-class SCCLvTrainer(nn.Module):
+insert_keyword = lambda dic, word: dict([(f"{word}/{k}", v) for k, v in dic.items()])
+format_float = lambda dic, ndigit: dict([(k, round(v, ndigit)) for k, v in dic.items()])
+class SCCLvTrainer(nn.Module): 
     def __init__(self, model, tokenizer, optimizer, train_loader, args):
         super(SCCLvTrainer, self).__init__()
         self.model = model
@@ -29,13 +34,32 @@ class SCCLvTrainer(nn.Module):
         self.train_loader = train_loader
         self.args = args
         self.eta = self.args.eta
-        
+        self.save_model_path = Path(__file__).parent.resolve() / "models" / self.args.dataname
+        self.save_interval = 100
+        self.best_model_scores = None
         self.cluster_loss = nn.KLDivLoss(size_average=False)
         self.contrast_loss = PairConLoss(temperature=self.args.temperature)
         
         self.gstep = 0
         print(f"*****Intialize SCCLv, temp:{self.args.temperature}, eta:{self.args.eta}\n")
-        
+    
+    def update_best_model(self, current_model):
+        if self.best_model_scores is None or current_model["avg"] > self.best_model_scores["best/avg"]:
+            # self.best_model_scores = copy.deepcopy(current_model)
+            self.best_model_scores =  dict([(f"best/{k}", v) for k, v in current_model.items()])
+            
+            wb.run.summary.update(self.best_model_scores)
+            wb.run.log(self.best_model_scores)
+            
+            path = self.save_model_path / f"{wb.run.id}-{self.args.bert}-best-model.pth"
+            torch.save(obj=self.model.state_dict(), f=path.__str__())
+
+
+            
+            return path
+        return None
+            # self.model()
+
     def get_batch_token(self, text):
         token_feat = self.tokenizer.batch_encode_plus(
             text, 
@@ -68,13 +92,14 @@ class SCCLvTrainer(nn.Module):
         return input_ids.cuda(), attention_mask.cuda()
         
         
-    def train_step_virtual(self, input_ids, attention_mask):
+    def train_step_virtual(self, input_ids, attention_mask, itr):
         
         embd1, embd2 = self.model(input_ids, attention_mask, task_type="virtual")
 
         # Instance-CL loss
         feat1, feat2 = self.model.contrast_logits(embd1, embd2)
         losses = self.contrast_loss(feat1, feat2)
+        constrastive_loss_value = losses["loss"].item()
         loss = self.eta * losses["loss"]
         
         # Clustering loss
@@ -82,61 +107,62 @@ class SCCLvTrainer(nn.Module):
             output = self.model.get_cluster_prob(embd1)
             target = target_distribution(output).detach()
             
-            cluster_loss = self.cluster_loss((output+1e-08).log(), target)/output.shape[0]
-            loss += 0.5*cluster_loss
-            losses["cluster_loss"] = cluster_loss.item()
+            cluster_loss = self.cluster_loss((output+1e-08).log(), target) / output.shape[0]
+            loss += 0.5 * cluster_loss
+            cluster_loss_value = cluster_loss.item()
+            losses["cluster_loss"] = cluster_loss_value
 
         loss.backward()
         self.optimizer.step()
         self.optimizer.zero_grad()
-        return losses
-    
-    
-    def train_step_explicit(self, input_ids, attention_mask):
-        
-        embd1, embd2, embd3 = self.model(input_ids, attention_mask, task_type="explicit")
 
-        # Instance-CL loss
-        feat1, feat2 = self.model.contrast_logits(embd2, embd3)
-        losses = self.contrast_loss(feat1, feat2)
-        loss = self.eta * losses["loss"]
+        wb_dic = {
+            "all-loss": loss.item(),
+            "contrast-loss": constrastive_loss_value,
+            "cluster-loss": cluster_loss_value,
+            "iter": itr
+            }
 
-        # Clustering loss
-        if self.args.objective == "SCCL":
-            output = self.model.get_cluster_prob(embd1)
-            target = target_distribution(output).detach()
-            
-            cluster_loss = self.cluster_loss((output+1e-08).log(), target)/output.shape[0]
-            loss += cluster_loss
-            losses["cluster_loss"] = cluster_loss.item()
-
-        loss.backward()
-        self.optimizer.step()
-        self.optimizer.zero_grad()
-        return losses
+        return losses, wb_dic
+    
     
     
     def train(self):
+        print("#" * 40)
         print('\n={}/{}=Iterations/Batches'.format(self.args.max_iter, len(self.train_loader)))
-
+        wb.watch(self.model)
         self.model.train()
         for i in np.arange(self.args.max_iter+1):
             try:
                 batch = next(train_loader_iter)
-            except:
+            except Exception as e:
+                print(e)
                 train_loader_iter = iter(self.train_loader)
                 batch = next(train_loader_iter)
 
             input_ids, attention_mask = self.prepare_transformer_input(batch)
 
-            losses = self.train_step_virtual(input_ids, attention_mask) if self.args.augtype == "virtual" else self.train_step_explicit(input_ids, attention_mask)
+            losses, loss_dic = self.train_step_virtual(input_ids, attention_mask, itr=i) if self.args.augtype == "virtual" else self.train_step_explicit(input_ids, attention_mask)
+            wb.run.log(loss_dic)
+            
 
-            if (self.args.print_freq>0) and ((i%self.args.print_freq==0) or (i==self.args.max_iter)):
+            if (self.args.print_freq > 0) and ((i%self.args.print_freq == 0)  or (i == self.args.max_iter)):
                 statistics_log(self.args.tensorboard, losses=losses, global_step=i)
-                self.evaluate_embedding(i)
+                repre_scores, model_scores = self.evaluate_embedding(i)
+                wb.run.log(repre_scores)
+                wb.run.log(model_scores)
+
+                best_path = self.update_best_model(model_scores)
+                if best_path is not None:
+                    print(f"###### new BEST step {i} ^^^ ######")
                 self.model.train()
+            
+            if i % self.save_interval == 0:
+                path = self.save_model_path / f"{wb.run.id}-{self.args.bert}-iter-{i}.pth"
+                torch.save(obj=self.model.state_dict(), f=path.__str__())
+            
 
-        return None   
+        return loss_dic, repre_scores, model_scores   
 
     
     def evaluate_embedding(self, step):
@@ -171,7 +197,7 @@ class SCCLvTrainer(nn.Module):
         kmeans = cluster.KMeans(n_clusters=self.args.num_classes, random_state=self.args.seed)
         embeddings = all_embeddings.cpu().numpy()
         kmeans.fit(embeddings)
-        pred_labels = torch.tensor(kmeans.labels_.astype(np.int))
+        pred_labels = torch.tensor(kmeans.labels_.astype(int))
         
         # clustering accuracy 
         confusion.add(pred_labels, all_labels)
@@ -191,12 +217,45 @@ class SCCLvTrainer(nn.Module):
         # np.save(self.args.resPath + 'embeddings_{}.npy'.format(step), embeddings)
         # np.save(self.args.resPath + 'labels_{}.npy'.format(step), all_labels.cpu())
 
-        print('[Representation] Clustering scores:',confusion.clusterscores()) 
-        print('[Representation] ACC: {:.3f}'.format(acc)) 
-        print('[Model] Clustering scores:',confusion_model.clusterscores()) 
-        print('[Model] ACC: {:.3f}'.format(acc_model))
-        return None
+        repre_scores = confusion.clusterscores()
+        model_scores = confusion.clusterscores()
+        repre_scores_rounded = format_float(dic=repre_scores, ndigit=4)
+        model_scores_rounded = format_float(dic=model_scores, ndigit=4)
+        
+        print(f'Iter {step}: [Representation]  scores: ', repre_scores_rounded) 
+        # print('[Representation] ACC: {:.3f}'.format(acc)) 
+        print(f'Iter {step}:          [Model]  scores: ', model_scores_rounded) 
+        # print('[Model] ACC: {:.3f}'.format(acc_model))
+
+        repre_scores_2 = insert_keyword(dic=repre_scores, word="repre")
+        model_scores_2 = insert_keyword(dic=model_scores, word="model")
+        repre_scores_2["iter"] = step
+        model_scores_2["iter"] = step
+        return repre_scores_2, model_scores_2
+
+
+
+    
+    def train_step_explicit(self, input_ids, attention_mask):
+        
+        embd1, embd2, embd3 = self.model(input_ids, attention_mask, task_type="explicit")
 
+        # Instance-CL loss
+        feat1, feat2 = self.model.contrast_logits(embd2, embd3)
+        losses = self.contrast_loss(feat1, feat2)
+        loss = self.eta * losses["loss"]
 
+        # Clustering loss
+        if self.args.objective == "SCCL":
+            output = self.model.get_cluster_prob(embd1)
+            target = target_distribution(output).detach()
+            
+            cluster_loss = self.cluster_loss((output+1e-08).log(), target)/output.shape[0]
+            loss += cluster_loss
+            losses["cluster_loss"] = cluster_loss.item()
 
-             
\ No newline at end of file
+        loss.backward()
+        self.optimizer.step()
+        self.optimizer.zero_grad()
+        return losses
+    
\ No newline at end of file
diff --git a/utils/kmeans.py b/utils/kmeans.py
index 1212357..d038fe0 100644
--- a/utils/kmeans.py
+++ b/utils/kmeans.py
@@ -10,8 +10,10 @@ import numpy as np
 from utils.metric import Confusion
 from sklearn.cluster import KMeans
 
+insert_keyword = lambda dic, word: dict([(f"{word}/{k}", v) for k, v in dic.items()])
+format_float = lambda dic, ndigit: dict([(k, round(v, ndigit)) for k, v in dic.items()])
 
-def get_mean_embeddings(bert, input_ids, attention_mask):
+def get_mean_embeddings(bert, input_ids, attention_mask, input_type_id, token_type_ids=None):
         bert_output = bert.forward(input_ids=input_ids, attention_mask=attention_mask)
         attention_mask = attention_mask.unsqueeze(-1)
         mean_output = torch.sum(bert_output[0]*attention_mask, dim=1) / torch.sum(attention_mask, dim=1)
@@ -29,7 +31,7 @@ def get_batch_token(tokenizer, text, max_length):
     return token_feat
 
 
-def get_kmeans_centers(bert, tokenizer, train_loader, num_classes, max_length):
+def get_kmeans_centers(bert, tokenizer, train_loader, num_classes, max_length, keyword):
     for i, batch in enumerate(train_loader):
 
         text, label = batch['text'], batch['label']
@@ -51,13 +53,15 @@ def get_kmeans_centers(bert, tokenizer, train_loader, num_classes, max_length):
 
     true_labels = all_labels
     pred_labels = torch.tensor(cluster_assignment)    
-    print("all_embeddings:{}, true_labels:{}, pred_labels:{}".format(all_embeddings.shape, len(true_labels), len(pred_labels)))
+    print("{}: all_embeddings:{}, true_labels:{}, pred_labels:{}".format(keyword, all_embeddings.shape, len(true_labels), len(pred_labels)))
 
     confusion.add(pred_labels, true_labels)
     confusion.optimal_assignment(num_classes)
-    print("Iterations:{}, Clustering ACC:{:.3f}, centers:{}".format(clustering_model.n_iter_, confusion.acc(), clustering_model.cluster_centers_.shape))
+    km_scores = confusion.clusterscores()
+    print("{}, Iterations:{}, Clustering ACC:{:.3f}, centers:{}".format(keyword, clustering_model.n_iter_, confusion.acc(), clustering_model.cluster_centers_.shape))
     
-    return clustering_model.cluster_centers_
+    worded_km_scores = insert_keyword(dic=km_scores, word=keyword)
+    return clustering_model.cluster_centers_, worded_km_scores
 
 
 
diff --git a/utils/metric.py b/utils/metric.py
index 49c7853..ab7734d 100755
--- a/utils/metric.py
+++ b/utils/metric.py
@@ -95,7 +95,9 @@ class Confusion(object):
             _,pred = output.max(1) #find the predicted class
         else: #it is already the predicted class
             pred = output
-        indices = (target*self.conf.stride(0) + pred.squeeze_().type_as(target)).type_as(self.conf)
+
+        indices = ((target - 1) * self.conf.stride(0) + pred.squeeze_().type_as(target)).type_as(self.conf)
+        # indices = (target*self.conf.stride(0) + pred.squeeze_().type_as(target)).type_as(self.conf)
         ones = torch.ones(1).type_as(self.conf).expand(indices.size(0))
         self._conf_flat = self.conf.view(-1)
         self._conf_flat.index_add_(0, indices, ones)
@@ -182,7 +184,12 @@ class Confusion(object):
     
     def clusterscores(self):
         target,pred = self.conf2label()
+        ACC = self.acc()
         NMI = normalized_mutual_info_score(target,pred)
         ARI = adjusted_rand_score(target,pred)
         AMI = adjusted_mutual_info_score(target,pred)
-        return {'NMI':NMI,'ARI':ARI,'AMI':AMI}
+        return {'ACC': ACC,
+                'NMI': NMI,
+                'ARI': ARI,
+                'AMI': AMI,
+                'avg': (ACC + NMI)/2}
diff --git a/utils/optimizer.py b/utils/optimizer.py
index 5cd2210..880f6d6 100644
--- a/utils/optimizer.py
+++ b/utils/optimizer.py
@@ -17,6 +17,7 @@ BERT_CLASS = {
 
 SBERT_CLASS = {
     "distilbert": 'distilbert-base-nli-stsb-mean-tokens',
+    "minilm6": "all-MiniLM-L6-v2"
 }
 
 
@@ -24,8 +25,8 @@ def get_optimizer(model, args):
     
     optimizer = torch.optim.Adam([
         {'params':model.bert.parameters()}, 
-        {'params':model.contrast_head.parameters(), 'lr': args.lr*args.lr_scale},
-        {'params':model.cluster_centers, 'lr': args.lr*args.lr_scale}
+        {'params':model.contrast_head.parameters(), 'lr': args.lr * args.lr_scale},
+        {'params':model.cluster_centers, 'lr': args.lr * args.lr_scale}
     ], lr=args.lr)
     
     print(optimizer)
@@ -49,7 +50,7 @@ def get_bert(args):
 
 
 def get_sbert(args):
-    sbert = SentenceTransformer(SBERT_CLASS[args.bert])
+    sbert = SentenceTransformer(SBERT_CLASS[args.bert], device="cuda")
     return sbert
 
 
